## Tasks
[ ] Create a python producer to mock a continuous producer of the data (like CPU data, or  Memory utilisation, Network rate)
[ ] Build a python consumer to consume data from kafka stream, analysing and outputs the result to another kafka stream
[ ] Configure logstash to fetch and persist data in the Elasticsearch database
[ ] Setup a kafka monitoring dashboard in prometheus or landoop
[ ] Implement a Apache spark job, which process data from kafka
[ ] Implement a Apache spark job, which uses grpahix
[ ] Implement a python stream processing job, which process the data using KSQL


https://www.pavanpkulkarni.com/blog/13-spark-on-docker/
https://medium.com/@thiagolcmelo/submitting-a-python-job-to-apache-spark-on-docker-b2bd19593a06

https://towardsdatascience.com/how-does-apache-spark-run-on-a-cluster-974ec2731f20
